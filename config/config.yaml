defaults:
  - override hydra/launcher: submitit_slurm # Configure a Hydra launcher to parse multiple parts in parallel, for SLURM use e.g.: https://hydra.cc/docs/plugins/submitit_launcher/

dataset_path: /ivi/ilps/datasets/baidu_ultr/ # Input directory with part-*.gz files
output_path: /ivi/ilps/datasets/baidu_ultr_small/ # Output directory to store .parquet files
part: 0 # Specific part number to load, sweep over multiple parts with python parse.py -m 'part=range(0,2000)'

### Train columns to load ###
train_query_columns:
  - qid
  - query

train_document_columns:
  - pos
  - click
  - displayed_count
  - title

### Test columns to load ###
test_columns:
  - query
  - title
  - label

### Post-process ###
train_document_pipeline:
  _target_: src.processing.Pipeline
  steps:
    - _target_: src.processing.HashTokens
      column: title
      unique_tokens: False
      drop_column: True

train_query_pipeline:
  _target_: src.processing.Pipeline
  steps:
    - _target_: src.processing.HashTokens
      column: query
      unique_tokens: False
      drop_column: True

test_pipeline:
  _target_: src.processing.Pipeline
  steps:
    - _target_: src.processing.HashTokens
      column: query
      unique_tokens: False
      drop_column: True
    - _target_: src.processing.HashTokens
      column: title
      unique_tokens: False
      drop_column: True

# Delete temporary .parquet files created per part after merging into a single output file.
# Not deleting parts might be beneficial when debugging post-processing pipelines.
delete_parts_after_merge: True

hydra:
  launcher:
    mem_gb: 4
    cpus_per_task: 4
    array_parallelism: 20
    partition: cpu
